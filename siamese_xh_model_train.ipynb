{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten, Dropout\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPool2D, Flatten, Activation, Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "model_path = 'model/siamesemodel_xhlayer_224_dataaugmented_epoch40_299samples_b16.h5'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 70\n",
    "\n",
    "anchor_dir = 'train_data/anchor'\n",
    "positive_dir = 'train_data/positive'\n",
    "negative_dir = 'train_data_yx/negative_lfw'\n",
    "\n",
    "\n",
    "# Load negative data into Tensorflow dataset\n",
    "def load_data(filepath, folder_no):\n",
    "    directories = os.listdir(filepath)\n",
    "    all_data = []\n",
    "\n",
    "    for directory in directories:\n",
    "        current_path = os.path.join(filepath, directory)\n",
    "        load_data = tf.data.Dataset.list_files(current_path + '/*.*').take(folder_no)\n",
    "        dir_iterator = load_data.as_numpy_iterator()\n",
    "        folder_data = [dir_iterator.next() for _ in range(len(load_data))]\n",
    "        all_data.extend(folder_data)\n",
    "\n",
    "        # Generate tensorflow dataset\n",
    "        load_data = tf.data.Dataset.from_tensor_slices(all_data)\n",
    "\n",
    "    return load_data\n",
    "\n",
    "\n",
    "# Image preprocessing\n",
    "def preprocessing(filepath):\n",
    "    byte_img = tf.io.read_file(filepath)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "# Function to preprocess image for twin pairs\n",
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    input_img = preprocessing(input_img)\n",
    "    validation_img = preprocessing(validation_img)\n",
    "    return (input_img, validation_img, label)\n",
    "\n",
    "\n",
    "data_augmentation_layer = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.Lambda(lambda x: tf.image.adjust_brightness(x, tf.random.uniform(shape=(), minval=0.05, maxval=0.15))),\n",
    "    layers.Rescaling(scale=0.8)\n",
    "])\n",
    "\n",
    "# Create a data augmentation layer\n",
    "def random_augmentation(x):\n",
    "    augmented_img = tf.cond(\n",
    "        tf.greater(tf.random.uniform(shape=(), minval=0, maxval=1), 0.5),\n",
    "        lambda: data_augmentation_layer(x),\n",
    "        lambda: x\n",
    "        )\n",
    "\n",
    "    return augmented_img\n",
    "\n",
    "\n",
    "# Categorise train dataset with augmentation\n",
    "def create_train_dataset(data, batch=16, prefetch=8):\n",
    "    train_data = data.take(round(len(data) * 0.7))\n",
    "    train_data = train_data.cache()\n",
    "    train_data = train_data.shuffle(buffer_size=len(train_data))\n",
    "    train_data = train_data.map(lambda x, y, z: (random_augmentation(x), random_augmentation(y), z))\n",
    "    train_data = train_data.batch(batch)\n",
    "    train_data = train_data.prefetch(prefetch)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "\n",
    "# Categorise test dataset\n",
    "def create_test_dataset(data, batch=16, prefetch=8):\n",
    "    test_data=data.skip(round(len(data) * 0.7))\n",
    "    test_data=test_data.take(round(len(data)*0.3))\n",
    "    test_data=test_data.batch(batch)\n",
    "    test_data=test_data.prefetch(prefetch)\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "# Display augmented image\n",
    "def compare_augmented_images(augmented_samples):\n",
    "    for i, (input_img, validation_img, label) in enumerate(augmented_samples):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(input_img[0])\n",
    "        plt.title(f'Input Image - Label: {label.numpy()}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(validation_img[0])\n",
    "        plt.title(f'Validation Image - Label: {label.numpy()}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Caluclate L1 (Manhattan) Distance\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n",
    "\n",
    "\n",
    "# Define CNN for embedding using VGGFace\n",
    "def embedding_model():\n",
    "    inp = Input(shape=(224, 224, 3))\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu')(inp)\n",
    "    m1 = MaxPool2D((2, 2), padding='same')(c1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu')(m1)\n",
    "    m2 = MaxPool2D((2, 2), padding='same')(c2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu')(m2)\n",
    "    m3 = MaxPool2D((2, 2), padding='same')(c3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu')(m3)\n",
    "    m4 = MaxPool2D((2, 2), padding='same')(c4)\n",
    "    c5 = Conv2D(512, (3, 3), activation='relu')(m4)\n",
    "    m5 = MaxPool2D((2, 2), padding='same')(c5)\n",
    "\n",
    "    f1 = Flatten()(m5)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Compare similarity using two pipelines\n",
    "def siamese_model(embedding):\n",
    "    # define two input tensors\n",
    "    input_image = Input(name='input_img', shape=(224, 224, 3))\n",
    "    validation_image = Input(name='validation_img', shape=(224, 224, 3))\n",
    "\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Apply dense layer with single neuron for similarity classifier\n",
    "    # Sigmoid to compress single input to 0 and 1 as output\n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "\n",
    "    siamese_model = keras.Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNeuralNetwork')\n",
    "    return siamese_model\n",
    "\n",
    "\n",
    "@tf.function\n",
    "# Execute single training step for Siamese Neural Network\n",
    "def train_step(batch, siamese_model, optimizer, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x = batch[:2]       # Input 1 and 2\n",
    "        y = batch[2]        # Label\n",
    "        yhat = siamese_model(x, training=True)\n",
    "        loss = loss_fn(y, yhat)\n",
    "\n",
    "    # Compute loss gradient for variables optimization\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Siamese Neural Network\n",
    "def train_model(siamese_model, train_data, test_data, optimizer, loss_fn, EPOCHS, early_stopping=None):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    callbacks = [early_stopping] if early_stopping else None\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f'\\nEpoch {epoch}/{EPOCHS}')\n",
    "        progbar1 = tf.keras.utils.Progbar(len(train_data))\n",
    "        progbar2 = tf.keras.utils.Progbar(len(test_data))\n",
    "\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "        for idx1, batch in enumerate(train_data):\n",
    "            loss = train_step(batch, siamese_model, optimizer, loss_fn)\n",
    "            y_true = batch[2].numpy()\n",
    "            y_pred = siamese_model.predict(batch[:2])\n",
    "            y_pred = (y_pred > 0.5).astype(int)\n",
    "            accuracy = np.mean(y_true == y_pred.flatten())\n",
    "\n",
    "            epoch_losses.append(loss.numpy())\n",
    "            epoch_accuracies.append(accuracy)\n",
    "\n",
    "        val_epoch_losses = []\n",
    "        val_epoch_accuracies = []\n",
    "        for idx2, val_batch in enumerate(test_data):\n",
    "            val_loss = loss_fn(siamese_model(val_batch[:2], training=False), val_batch[2])\n",
    "            y_true_val = val_batch[2].numpy()\n",
    "            y_pred_val = siamese_model.predict(val_batch[:2])\n",
    "            y_pred_val = (y_pred_val > 0.5).astype(int)\n",
    "            val_accuracy = np.mean(y_true_val == y_pred_val.flatten())\n",
    "\n",
    "            val_epoch_losses.append(val_loss.numpy())\n",
    "            val_epoch_accuracies.append(val_accuracy)\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_accuracy = np.mean(epoch_accuracies)\n",
    "        print(f'\\nEpoch {epoch} - Average Training Loss: {avg_loss:.4f} - Average Training Accuracy: {avg_accuracy:.4f}')\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(avg_accuracy)\n",
    "\n",
    "        avg_val_loss = np.mean(val_epoch_losses)\n",
    "        avg_val_accuracy = np.mean(val_epoch_accuracies)\n",
    "        print(f'Epoch {epoch} - Average Validation Loss: {avg_val_loss:.4f} - Average Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(avg_val_accuracy)\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if early_stopping and early_stopping.stopped_epoch > 0:\n",
    "            print(f'Early stopping triggered. Restoring model weights from epoch {early_stopping.stopped_epoch}.')\n",
    "            break\n",
    "\n",
    "        progbar1.update(idx1 + 1)\n",
    "        progbar2.update(idx2 + 1)\n",
    "\n",
    "    # Plot the loss, accuracy, validation loss, and validation accuracy upon training completion\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(losses) + 1), losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(accuracies) + 1), accuracies, label='Training Accuracy')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate_metrics(siamese_model, test_data):\n",
    "    test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n",
    "    y_pred = siamese_model.predict([test_input, test_val])\n",
    "\n",
    "    precision_metric = Precision()\n",
    "    precision_metric.update_state(y_true, y_pred)\n",
    "    precision_result = precision_metric.result().numpy()\n",
    "\n",
    "    recall_metric = Recall()\n",
    "    recall_metric.update_state(y_true, y_pred)\n",
    "    recall_result = recall_metric.result().numpy()\n",
    "\n",
    "    print('\\nEvaluation Results:')\n",
    "    print(f'Precision: {precision_result:.4f}')\n",
    "    print(f'Recall: {recall_result:.4f}')\n",
    "\n",
    "\n",
    "# Prepare tensorflow dataset for anchor, positive and negative\n",
    "# Load datasets\n",
    "pos_path = '/content/drive/MyDrive/Siamese_Neural_Network_Train/train_data/positive'\n",
    "anchor_path = '/content/drive/MyDrive/Siamese_Neural_Network_Train/train_data/anchor'\n",
    "# neg_path = os.path.join('train_data', 'negative')\n",
    "\n",
    "# Initialize a list to store all data\n",
    "all_data = []\n",
    "\n",
    "# Loop over celebrity folders in positive directory\n",
    "celeb_folders = os.listdir(pos_path)\n",
    "\n",
    "print(celeb_folders)\n",
    "for celeb_folder in celeb_folders:\n",
    "    # print(celeb_folder)\n",
    "\n",
    "    celeb_path = os.path.join(pos_path, celeb_folder)\n",
    "\n",
    "    # Take positive samples from the current celebrity folder\n",
    "    positive_samples = tf.data.Dataset.list_files(celeb_path + '/*.*').take(95)\n",
    "\n",
    "    # Take anchor samples from the anchor folder (same person)\n",
    "    anchor_samples = tf.data.Dataset.list_files(os.path.join(anchor_path, celeb_folder) + '/*.*').take(95)\n",
    "\n",
    "    # Create pairs of anchor, positive, and label (1 for same person, 0 for different persons)\n",
    "    positive_pairs = tf.data.Dataset.zip((anchor_samples, positive_samples, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor_samples)))))\n",
    "\n",
    "    all_data.append(positive_pairs)\n",
    "\n",
    "    # Take negative samples from other celebrity folders in positive_path (different person)\n",
    "    other_celebs = [folder for folder in celeb_folders if folder != celeb_folder]\n",
    "    # print(other_celebs)\n",
    "    for otherpath in other_celebs:  # Repeat for each remaining celebrity folder\n",
    "        # Randomly select a different celebrity folder for negative samples\n",
    "        other_celeb_path = os.path.join(pos_path, otherpath)\n",
    "\n",
    "        # Take positive samples from the other celebrity folder\n",
    "        other_positive_samples = tf.data.Dataset.list_files(other_celeb_path + '/*.*',shuffle=True).take(5)\n",
    "\n",
    "        # Use the same set of anchor images for the length of other_positive_samples\n",
    "        additional_anchor_samples = anchor_samples.take(len(other_positive_samples))\n",
    "\n",
    "        # Create pairs of anchor, negative, and label (0 for different persons)\n",
    "        negative_pairs = tf.data.Dataset.zip((additional_anchor_samples, other_positive_samples, tf.data.Dataset.from_tensor_slices(tf.zeros(len(additional_anchor_samples)))))\n",
    "\n",
    "        # Append positive and negative pairs to the list\n",
    "        all_data.append(negative_pairs)\n",
    "\n",
    "# Concatenate all datasets in the list to produce a single dataset\n",
    "final_dataset = all_data[0]\n",
    "for dataset in all_data[1:]:\n",
    "    final_dataset = final_dataset.concatenate(dataset)\n",
    "\n",
    "data=final_dataset\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=len(data))\n",
    "\n",
    "train_data = create_train_dataset(data, BATCH_SIZE, 8)\n",
    "test_data = create_test_dataset(data, BATCH_SIZE, 8)\n",
    "\n",
    "for item in data.take(10):  # Take the first two indices\n",
    "    anchor_img, positive_img, label = item\n",
    "\n",
    "    # Convert images back to numpy array for plotting\n",
    "    anchor_img = anchor_img.numpy()\n",
    "    positive_img = positive_img.numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(anchor_img)\n",
    "    plt.title(\"Anchor Image\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(positive_img)\n",
    "    if label == 1:\n",
    "        plt.title(\"Positive Image\")\n",
    "    else:\n",
    "        plt.title(\"Negative Image\")\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Length of Total Data : {len(data)}\")\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Test Data  : {len(test_data)}\")\n",
    "\n",
    "augmented_samples = train_data.take(20)\n",
    "compare_augmented_images(augmented_samples)\n",
    "\n",
    "embedding = embedding_model()\n",
    "siamese_model = siamese_model(embedding)\n",
    "binary_cross_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam(1e-5)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)\n",
    "\n",
    "train_model(siamese_model, train_data, test_data, opt, binary_cross_loss, EPOCH)\n",
    "\n",
    "evaluate_metrics(siamese_model, test_data)\n",
    "siamese_model.save(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
